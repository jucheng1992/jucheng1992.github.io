# jemdoc: menu{MENU}{index.html}, analytics{UA-44238171-1},
# The first line of this file is a special command that tells jemdoc which menu
# entry in the file named MENU to associate this page with.

\n

= Assignment 0: Describe a Parallel Application

== Brief bio

I’m Cheng Ju, a third year Ph.D. at UC Berkeley majoring in Biostatistics. I’m majorly interested in machine learning with its applications in causal inference. I’d like to master some parallel programming skills and probably build efficient software/applications in machine learning, and solve some big data analytics problem in public health.

=== Application problem: Deep Learning for Computer Vision

An application of parallel computing is image recognition in computer vision, which involves training/tuning deep learning systems, such as convolutional neural networks (CNN) and recurrent neural networks, parallelly.

=== What is the scientific or engineering problem being solved?

For image recognition/classification, a deep learning system usually involves a training dataset of over 1 million images. In addition, the model have large size, with millions of parameters, which involves a lot of computation.

=== How well did the application achieve its scientific/engineering objective? Are simulation results compared to physical results?

This is achieved by parallelism of multiple GPUS.

=== What parallel platform has the application targeted? (distributed vs. shared memory, vector, etc.) What tools were used to build the application? (languages, libraries, etc.)

This application mainly target in the GPU computation. Usually it runs on a single machine with multiple GPUs, and communicates with each other using Nvidia’s P2P DMA access.


=== If the application is run on a major supercomputer, where does that computer rank on the Top 500 list?

No, it is not run on a supercomputer.

=== How well did the application perform? How does this compare to the platform's best possible performance?


=== Does the application "scale" to large problems on many processors? If you believe it has not, what bottlenecks may have limited its performance?

It is much faster compared to previous single-GPU training model. However, it is still not "sacle". For example, it still usually takes several weeks to train a deep convolutional neural network (e.g. ResNet) on ImageNet dataset. There are several bottlenecks: First, moving data in and out of GPU requires a lot more time. Second, training a deep neural network is a highly non-convex problem. It is hard to converge, especially with asynchronous training, which is widely used in distributed optimization.




 